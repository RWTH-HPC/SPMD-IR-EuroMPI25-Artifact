[MUST] MUST configuration ... distributed checks without application crash handling
[MUST] Information: overwritting old intermediate data in directory "/tmp/tmp.RDisHO1Y0R"!
[MUST] Using prebuilt infrastructure at /opt/rmasanitizer/modules/prebuilds/dd3dc15ec93989ca88e2155a5c3c2d31
[MUST] Weaver ... success
[MUST] Generating P^nMPI configuration ... success
[MUST] Infrastructure in "/opt/rmasanitizer/modules/prebuilds/dd3dc15ec93989ca88e2155a5c3c2d31" is present and used.
[MUST] Search for linked P^nMPI ... not found ... using LD_PRELOAD to load P^nMPI ... success
[MUST] Note: Full ThreadSanitizer support requires the application to be built with either a GNU-based compiler in version 9 to 11 or an LLVM-based compiler in version 6 to 13.
[MUST] Note: MPI RMA support requires the application to be built with Clang >= 15.Note: MPI RMA support requires the application to be built with Clang >= 15.Executing application:
[MUST-RUNTIME] Could not find TSan sync clock for vector clock value 2
[MUST-RUNTIME] Could not find location for vector clock value 2
Process 0: Execution finished, variable contents: value = 4, value2 = 2, win_base[0] = 0
Process 1: Execution finished, variable contents: value = 1, value2 = 2, win_base[0] = 0
[MUST-REPORT] Error: from: call @1: Remote data race at rank 1 between a write of size 4 at MPI_Put@1 from rank 0 and a previous write of size 4 at main@2 from rank 1. Concurrent region of reference 1 started at reference 3 and ended at reference 4. 
[MUST-REPORT]  References of a representative process:
[MUST-REPORT] Reference 1: call MPI_Putmain
[MUST-REPORT] /rwthfs/rz/cluster/work/rwth1282/spmdir/spmd/externals/data-race-detection-benchmark-suite/cq-results-20250731-154348/RRB/MPIRMA/RMASanitizer-NoOpt/rma+nonrma/013-MPI-coll-put-ireduce-wait-local-no.c:41
[MUST-REPORT]  @rank 0, threadid 1;
[MUST-REPORT] Reference 2: call main
[MUST-REPORT] /rwthfs/rz/cluster/work/rwth1282/spmdir/spmd/externals/data-race-detection-benchmark-suite/cq-results-20250731-154348/RRB/MPIRMA/RMASanitizer-NoOpt/rma+nonrma/013-MPI-coll-put-ireduce-wait-local-no.c:33
[MUST-REPORT]  @rank 1, threadid 0;
[MUST-REPORT] Reference 3: call main
[MUST-REPORT] /rwthfs/rz/cluster/work/rwth1282/spmdir/spmd/externals/data-race-detection-benchmark-suite/cq-results-20250731-154348/RRB/MPIRMA/RMASanitizer-NoOpt/rma+nonrma/013-MPI-coll-put-ireduce-wait-local-no.c:33
[MUST-REPORT]  @rank 1, threadid 0;
[MUST-REPORT] Reference 4: call MPI_Win_fencemain
[MUST-REPORT] /rwthfs/rz/cluster/work/rwth1282/spmdir/spmd/externals/data-race-detection-benchmark-suite/cq-results-20250731-154348/RRB/MPIRMA/RMASanitizer-NoOpt/rma+nonrma/013-MPI-coll-put-ireduce-wait-local-no.c:45
[MUST-REPORT]  @rank 1, threadid 0;
[MUST] Execution finished.
